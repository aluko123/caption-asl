setting asax scratch directory
 
============================================================
=====         Summary of your script job               =====
============================================================
  The script file is: run_analysis.sh
  The time limit is 4:00:00 HH:MM:SS.
  The target directory is: /home/uahoxa001/WLASL/start_kit
  The working directory is:  /scratch-local/uahoxa001.model2.95257.asax-pbs1
  The memory limit is: 16gb
  The job will start running after: 202407300922.55
  Job Name: model2
  Queue: -q express
  Constraints: 
  Using  4  cores on master node  asax002.asc.edu
  Node list:  asax002.asc.edu asax002.asc.edu asax002.asc.edu asax002.asc.edu
  Cores:  4
  Command typed:
/scripts/run_script run_analysis.sh     
  Queue submit command:
qsub -q express -j oe -N model2 -a 202407300922.55 -r n -M oaa0008@uah.edu -l walltime=4:00:00 -l select=ncpus=4:mpiprocs=4:mem=16000mb 

Lmod is automatically replacing "anaconda/3-2024.02" with "pytorch/2019".

paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress
Top 10 most common signs:
thin: 15
go: 14
computer: 13
before: 13
help: 13
cool: 13
bowling: 13
drink: 12
thanksgiving: 12
bed: 12
Cluster 0: ['brown', 'pants', 'chicken', 'water', 'bag']
Cluster 1: ['too', 'one', 'all', 'just', 'a']
Cluster 2: ['go', 'make', 'take', 'get', 'bring']
Cluster 3: ['school', 'town', 'city', 'thing', 'house']
Cluster 4: ['america', 'europe', 'england', 'usa', 'france']

Most similar signs:
Signs similar to 'book': ['books', 'author', 'memoir', 'Book', 'tome']
Signs similar to 'drink': ['drinks', 'drinking', 'Drink', 'beverage', 'drank']
Signs similar to 'computer': ['computers', 'Computer', 'software', 'laptop', 'computing']
Vocabulary size: 2003
[nltk_data] Error loading punkt: <urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:852)>
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify
[nltk_data]     failed (_ssl.c:852)>
Generated 1600 training samples and 400 test samples.

Example parallel data:
English: The business is here
ASL Gloss: BUSINESS HERE

English: The raccoon is here
ASL Gloss: RACCOON HERE

English: It is daily
ASL Gloss: IT DAILY

English: The solve is here
ASL Gloss: SOLVE HERE

English: The desk is here
ASL Gloss: DESK HERE

Sample src: tensor([2000,  188, 2000,  212])
Sample trg: tensor([2000, 2000])
Vocabulary size: 2003
Epoch: 1, Batch: 0, Loss: 7.6306
Epoch: 1, Batch: 10, Loss: 0.0005
Epoch: 1, Batch: 20, Loss: 0.0000
Epoch: 1, Batch: 30, Loss: 0.0000
Epoch: 1, Batch: 40, Loss: 0.0000
Epoch: 1, Batch: 50, Loss: 0.0000
Epoch: 1, Batch: 60, Loss: 0.0000
Epoch: 1, Batch: 70, Loss: 0.0000
Epoch: 1, Batch: 80, Loss: 0.0000
Epoch: 1, Batch: 90, Loss: 0.0000
Epoch: 1, Batch: 100, Loss: 0.0000
Epoch: 1, Batch: 110, Loss: 0.0000
Epoch: 1, Batch: 120, Loss: 0.0000
Epoch: 1, Batch: 130, Loss: 0.0000
Epoch: 1, Batch: 140, Loss: 0.0000
Epoch: 1, Batch: 150, Loss: 0.0000
Epoch: 1, Batch: 160, Loss: 0.0000
Epoch: 1, Batch: 170, Loss: 0.0000
Epoch: 1, Batch: 180, Loss: 0.0000
Epoch: 1, Batch: 190, Loss: 0.0000
Epoch: 1, Batch: 200, Loss: 0.0000
Epoch: 1, Batch: 210, Loss: 0.0000
Epoch: 1, Batch: 220, Loss: 0.0000
Epoch: 1, Batch: 230, Loss: 0.0000
Epoch: 1, Batch: 240, Loss: 0.0000
Epoch: 1, Batch: 250, Loss: 0.0000
Epoch: 1, Batch: 260, Loss: 0.0000
Epoch: 1, Batch: 270, Loss: 0.0000
Epoch: 1, Batch: 280, Loss: 0.0000
Epoch: 1, Batch: 290, Loss: 0.0000
Epoch: 1, Batch: 300, Loss: 0.0000
Epoch: 1, Batch: 310, Loss: 0.0000
Epoch: 1, Batch: 320, Loss: 0.0000
Epoch: 1, Batch: 330, Loss: 0.0000
Epoch: 1, Batch: 340, Loss: 0.0000
Epoch: 1, Batch: 350, Loss: 0.0000
Epoch: 1, Batch: 360, Loss: 0.0000
Epoch: 1, Batch: 370, Loss: 0.0000
Epoch: 1, Batch: 380, Loss: 0.0000
Epoch: 1, Batch: 390, Loss: 0.0000
Epoch: 1, Batch: 400, Loss: 0.0000
Epoch: 1, Batch: 410, Loss: 0.0000
Epoch: 1, Batch: 420, Loss: 0.0000
Epoch: 1, Batch: 430, Loss: 0.0000
Epoch: 1, Batch: 440, Loss: 0.0000
Epoch: 1, Batch: 450, Loss: 0.0000
Epoch: 1, Batch: 460, Loss: 0.0000
Epoch: 1, Batch: 470, Loss: 0.0000
Epoch: 1, Batch: 480, Loss: 0.0000
Epoch: 1, Batch: 490, Loss: 0.0000
Epoch: 1, Batch: 500, Loss: 0.0000
Epoch: 1, Batch: 510, Loss: 0.0000
Epoch: 1, Batch: 520, Loss: 0.0000
Epoch: 1, Batch: 530, Loss: 0.0000
Epoch: 1, Batch: 540, Loss: 0.0000
Epoch: 1, Batch: 550, Loss: 0.0000
Epoch: 1, Batch: 560, Loss: 0.0000
Epoch: 1, Batch: 570, Loss: 0.0000
Epoch: 1, Batch: 580, Loss: 0.0000
Epoch: 1, Batch: 590, Loss: 0.0000
Epoch: 1, Batch: 600, Loss: 0.0000
Epoch: 1, Batch: 610, Loss: 0.0000
Epoch: 1, Batch: 620, Loss: 0.0000
Epoch: 1, Batch: 630, Loss: 0.0000
Epoch: 1, Batch: 640, Loss: 0.0000
Epoch: 1, Batch: 650, Loss: 0.0000
Epoch: 1, Batch: 660, Loss: 0.0000
Epoch: 1, Batch: 670, Loss: 0.0000
Epoch: 1, Batch: 680, Loss: 0.0000
Epoch: 1, Batch: 690, Loss: 0.0000
Epoch: 1, Batch: 700, Loss: 0.0000
Epoch: 1, Batch: 710, Loss: 0.0000
Epoch: 1, Batch: 720, Loss: 0.0000
Epoch: 1, Batch: 730, Loss: 0.0000
Epoch: 1, Batch: 740, Loss: 0.0000
Epoch: 1, Batch: 750, Loss: 0.0000
Epoch: 1, Batch: 760, Loss: 0.0000
Epoch: 1, Batch: 770, Loss: 0.0000
Epoch: 1, Batch: 780, Loss: 0.0000
Epoch: 1, Batch: 790, Loss: 0.0000
Epoch: 1, Batch: 800, Loss: 0.0000
Epoch: 1, Batch: 810, Loss: 0.0000
Epoch: 1, Batch: 820, Loss: 0.0000
Epoch: 1, Batch: 830, Loss: 0.0000
Epoch: 1, Batch: 840, Loss: 0.0000
Epoch: 1, Batch: 850, Loss: 0.0000
Epoch: 1, Batch: 860, Loss: 0.0000
Epoch: 1, Batch: 870, Loss: 0.0000
Epoch: 1, Batch: 880, Loss: 0.0000
Epoch: 1, Batch: 890, Loss: 0.0000
Epoch: 1, Batch: 900, Loss: 0.0000
Epoch: 1, Batch: 910, Loss: 0.0000
Epoch: 1, Batch: 920, Loss: 0.0000
Epoch: 1, Batch: 930, Loss: 0.0000
Epoch: 1, Batch: 940, Loss: 0.0000
Epoch: 1, Batch: 950, Loss: 0.0000
Epoch: 1, Batch: 960, Loss: 0.0000
Epoch: 1, Batch: 970, Loss: 0.0000
Epoch: 1, Batch: 980, Loss: 0.0000
Epoch: 1, Batch: 990, Loss: 0.0000
Epoch: 1, Batch: 1000, Loss: 0.0000
Epoch: 1, Batch: 1010, Loss: 0.0000
Epoch: 1, Batch: 1020, Loss: 0.0000
Epoch: 1, Batch: 1030, Loss: 0.0000
Epoch: 1, Batch: 1040, Loss: 0.0000
Epoch: 1, Batch: 1050, Loss: 0.0000
Epoch: 1, Batch: 1060, Loss: 0.0000
Epoch: 1, Batch: 1070, Loss: 0.0000
Epoch: 1, Batch: 1080, Loss: 0.0000
Epoch: 1, Batch: 1090, Loss: 0.0000
Epoch: 1, Batch: 1100, Loss: 0.0000
Epoch: 1, Batch: 1110, Loss: 0.0000
Epoch: 1, Batch: 1120, Loss: 0.0000
Epoch: 1, Batch: 1130, Loss: 0.0000
Epoch: 1, Batch: 1140, Loss: 0.0000
Epoch: 1, Batch: 1150, Loss: 0.0000
Epoch: 1, Batch: 1160, Loss: 0.0000
Epoch: 1, Batch: 1170, Loss: 0.0000
Epoch: 1, Batch: 1180, Loss: 0.0000
Epoch: 1, Batch: 1190, Loss: 0.0000
Epoch: 1, Batch: 1200, Loss: 0.0000
Epoch: 1, Batch: 1210, Loss: 0.0000
Epoch: 1, Batch: 1220, Loss: 0.0000
Epoch: 1, Batch: 1230, Loss: 0.0000
Epoch: 1, Batch: 1240, Loss: 0.0000
Epoch: 1, Batch: 1250, Loss: 0.0000
Epoch: 1, Batch: 1260, Loss: 0.0000
Epoch: 1, Batch: 1270, Loss: 0.0000
Epoch: 1, Batch: 1280, Loss: 0.0000
Epoch: 1, Batch: 1290, Loss: 0.0000
Epoch: 1, Batch: 1300, Loss: 0.0000
Epoch: 1, Batch: 1310, Loss: 0.0000
Epoch: 1, Batch: 1320, Loss: 0.0000
Epoch: 1, Batch: 1330, Loss: 0.0000
Epoch: 1, Batch: 1340, Loss: 0.0000
Epoch: 1, Batch: 1350, Loss: 0.0000
Epoch: 1, Batch: 1360, Loss: 0.0000
Epoch: 1, Batch: 1370, Loss: 0.0000
Epoch: 1, Batch: 1380, Loss: 0.0000
Epoch: 1, Batch: 1390, Loss: 0.0000
Epoch: 1, Batch: 1400, Loss: 0.0000
Epoch: 1, Batch: 1410, Loss: 0.0000
Epoch: 1, Batch: 1420, Loss: 0.0000
Epoch: 1, Batch: 1430, Loss: 0.0000
Epoch: 1, Batch: 1440, Loss: 0.0000
Epoch: 1, Batch: 1450, Loss: 0.0000
Epoch: 1, Batch: 1460, Loss: 0.0000
Epoch: 1, Batch: 1470, Loss: 0.0000
Epoch: 1, Batch: 1480, Loss: 0.0000
Epoch: 1, Batch: 1490, Loss: 0.0000
Epoch: 1, Batch: 1500, Loss: 0.0000
Epoch: 1, Batch: 1510, Loss: 0.0000
Epoch: 1, Batch: 1520, Loss: 0.0000
Epoch: 1, Batch: 1530, Loss: 0.0000
Epoch: 1, Batch: 1540, Loss: 0.0000
Epoch: 1, Batch: 1550, Loss: 0.0000
Epoch: 1, Batch: 1560, Loss: 0.0000
Epoch: 1, Batch: 1570, Loss: 0.0000
Epoch: 1, Batch: 1580, Loss: 0.0000
Epoch: 1, Batch: 1590, Loss: 0.0000
Epoch: 1, Batch: 1600, Loss: 0.0000
Epoch: 1, Batch: 1610, Loss: 0.0000
Epoch: 1, Batch: 1620, Loss: 0.0000
Epoch: 1, Batch: 1630, Loss: 0.0000
Epoch: 1, Batch: 1640, Loss: 0.0000
Epoch: 1, Batch: 1650, Loss: 0.0000
Epoch: 1, Batch: 1660, Loss: 0.0000
Epoch: 1, Batch: 1670, Loss: 0.0000
Epoch: 1, Batch: 1680, Loss: 0.0000
Epoch: 1, Batch: 1690, Loss: 0.0000
Epoch: 1, Batch: 1700, Loss: 0.0000
Epoch: 1, Batch: 1710, Loss: 0.0000
Epoch: 1, Batch: 1720, Loss: 0.0000
Epoch: 1, Batch: 1730, Loss: 0.0000
Epoch: 1, Batch: 1740, Loss: 0.0000
Epoch: 1, Batch: 1750, Loss: 0.0000
Epoch: 1, Batch: 1760, Loss: 0.0000
Epoch: 1, Batch: 1770, Loss: 0.0000
Epoch: 1, Batch: 1780, Loss: 0.0000
Epoch: 1, Batch: 1790, Loss: 0.0000
Epoch: 1, Batch: 1800, Loss: 0.0000
Epoch: 1, Batch: 1810, Loss: 0.0000
Epoch: 1, Batch: 1820, Loss: 0.0000
Epoch: 1, Batch: 1830, Loss: 0.0000
Epoch: 1, Batch: 1840, Loss: 0.0000
Epoch: 1, Batch: 1850, Loss: 0.0000
Epoch: 1, Batch: 1860, Loss: 0.0000
Epoch: 1, Batch: 1870, Loss: 0.0000
Epoch: 1, Batch: 1880, Loss: 0.0000
Epoch: 1, Batch: 1890, Loss: 0.0000
Epoch: 1, Batch: 1900, Loss: 0.0000/apps/x86-64/apps/pytorch_2019/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
  if np.issubdtype(vec.dtype, np.int):
/apps/x86-64/apps/pytorch_2019/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
  if np.issubdtype(vec.dtype, np.int):

Epoch: 1, Batch: 1910, Loss: 0.0000
Epoch: 1, Batch: 1920, Loss: 0.0000
Epoch: 1, Batch: 1930, Loss: 0.0000
Epoch: 1, Batch: 1940, Loss: 0.0000
Epoch: 1, Batch: 1950, Loss: 0.0000
Epoch: 1, Batch: 1960, Loss: 0.0000
Epoch: 1, Batch: 1970, Loss: 0.0000
Epoch: 1, Batch: 1980, Loss: 0.0000
Epoch: 1, Batch: 1990, Loss: 0.0000
Traceback (most recent call last):
  File "transform_encoder.py", line 219, in <module>
    print(f'Epoch: {epoch+1}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]}')
AttributeError: 'StepLR' object has no attribute 'get_last_lr'
